{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c47e74-7d82-42c4-86f3-345fda07d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRanker\n",
    "from math import log2\n",
    "\n",
    "w1 = 0.7\n",
    "w2 = 0.3\n",
    "N_COLD = 15\n",
    "SPLIT_Q = 0.8\n",
    "train_path = \"./data/raw/train.csv\"\n",
    "books_path = \"./data/raw/books.csv\"\n",
    "book_genres_path = \"./data/raw/book_genres.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0d8617-226e-4147-b0c5-27b493c8d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id               int64\n",
      "book_id               int64\n",
      "has_read              int64\n",
      "rating                int64\n",
      "timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "   user_id  book_id  has_read  rating           timestamp\n",
      "0     3870   310170         0       0 2008-04-27 21:06:16\n",
      "1     3870   306406         0       0 2008-06-07 11:51:01\n",
      "2     4091   195676         0       0 2008-08-06 00:40:55\n",
      "3     3870   554261         1       8 2008-08-07 09:16:12\n",
      "4     3870    33078         1       2 2008-08-07 09:17:20\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "\n",
    "print(train.dtypes.head())\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdbd70f-0848-4403-8b0f-93670d039df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   book_id  genre\n",
      "0       20    433\n",
      "1       35    141\n",
      "2       52   1251\n",
      "3       54    125\n",
      "4       69    127\n"
     ]
    }
   ],
   "source": [
    "books = pd.read_csv(books_path)\n",
    "book_genres = pd.read_csv(book_genres_path)\n",
    "\n",
    "books_main_genre = (\n",
    "    book_genres\n",
    "    .groupby('book_id')['genre_id']\n",
    "    .agg(lambda x: x.mode().iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "books_metadata_df = books[['book_id']].merge(books_main_genre, on='book_id', how='left')\n",
    "books_metadata_df = books_metadata_df.rename(columns={'genre_id': 'genre'})\n",
    "\n",
    "print(books_metadata_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9085f47f-d88c-4e4e-9af2-4708e147b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_split (80-й перцентиль): 2020-09-11 23:28:35\n",
      "Размер train_hist: (215249, 5)\n",
      "Размер val_period: (53812, 5)\n",
      "\n",
      "train_hist диапазон времени: 2008-04-27 21:06:16 → 2020-09-11 23:28:35\n",
      "val_period диапазон времени: 2020-09-11 23:30:30 → 2021-09-06 00:17:11\n"
     ]
    }
   ],
   "source": [
    "split_point = train['timestamp'].quantile(SPLIT_Q)\n",
    "print(\"T_split (80-й перцентиль):\", split_point)\n",
    "\n",
    "train_hist = train[train['timestamp'] <= split_point].copy()\n",
    "val_period = train[train['timestamp'] > split_point].copy()\n",
    "\n",
    "print(\"Размер train_hist:\", train_hist.shape)\n",
    "print(\"Размер val_period:\", val_period.shape)\n",
    "\n",
    "print(\"\\ntrain_hist диапазон времени:\",\n",
    "      train_hist['timestamp'].min(), \"→\", train_hist['timestamp'].max())\n",
    "print(\"val_period диапазон времени:\",\n",
    "      val_period['timestamp'].min(), \"→\", val_period['timestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45726a37-648b-47d1-b090-7eb51d51f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_history_and_popularity(df):\n",
    "    user_hist_books = (\n",
    "        df\n",
    "        .groupby('user_id')['book_id']\n",
    "        .agg(lambda x: set(x.tolist()))\n",
    "        .to_dict()\n",
    "    )\n",
    "    book_popularity = (\n",
    "        df\n",
    "        .groupby('book_id')['user_id']\n",
    "        .nunique()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    popular_books = book_popularity.index.to_numpy()\n",
    "    return user_hist_books, popular_books\n",
    "\n",
    "\n",
    "def sample_cold_candidates_for_user(user_id, user_hist_books, popular_books, n_cold=N_COLD):\n",
    "    seen = user_hist_books.get(user_id, set())\n",
    "    cold = []\n",
    "    for b in popular_books:\n",
    "        if b not in seen:\n",
    "            cold.append(b)\n",
    "            if len(cold) >= n_cold:\n",
    "                break\n",
    "    return cold\n",
    "\n",
    "\n",
    "def build_cold_candidates(users, user_hist_books, popular_books, n_cold=N_COLD):\n",
    "    rows = []\n",
    "    for u in users:\n",
    "        cold_books = sample_cold_candidates_for_user(u, user_hist_books, popular_books, n_cold=n_cold)\n",
    "        for b in cold_books:\n",
    "            rows.append((u, b, 0))\n",
    "    df = pd.DataFrame(rows, columns=['user_id', 'book_id', 'rel'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_basic_features(df_candidates, user_stats_df, book_stats_df):\n",
    "    df = df_candidates.copy()\n",
    "    df = df.merge(user_stats_df, on='user_id', how='left')\n",
    "    df = df.merge(book_stats_df, on='book_id', how='left')\n",
    "    df = df.fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa414b02-352f-467d-a636-97d1e076f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(rels, k=20):\n",
    "    rels = np.asarray(rels)[:k]\n",
    "    if rels.size == 0:\n",
    "        return 0.0\n",
    "    return float(sum(rel / log2(i + 2) for i, rel in enumerate(rels)))\n",
    "\n",
    "\n",
    "def ndcg_for_user(df_u, k=20):\n",
    "    df_sorted = df_u.sort_values('pred', ascending=False)\n",
    "    rels_pred = df_sorted['rel'].values\n",
    "    dcg = dcg_at_k(rels_pred, k=k)\n",
    "    ideal_rels = np.sort(df_u['rel'].values)[::-1]\n",
    "    idcg = dcg_at_k(ideal_rels, k=k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def mean_ndcg(df, k=20):\n",
    "    scores = []\n",
    "    for user_id, df_u in df.groupby('user_id'):\n",
    "        score_u = ndcg_for_user(df_u, k=k)\n",
    "        scores.append(score_u)\n",
    "    if not scores:\n",
    "        return 0.0\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def make_submission_user_list(df_pred, top_k=20):\n",
    "    submission_rows = []\n",
    "    for user_id, df_u in df_pred.groupby('user_id'):\n",
    "        df_u = df_u.drop_duplicates('book_id')\n",
    "        df_sorted = df_u.sort_values('pred', ascending=False)\n",
    "        top_books = df_sorted['book_id'].head(top_k).tolist()\n",
    "        book_id_list_str = \",\".join(map(str, top_books))\n",
    "        submission_rows.append((user_id, book_id_list_str))\n",
    "    sub = pd.DataFrame(submission_rows, columns=['user_id', 'book_id_list'])\n",
    "    return sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8c3400-db65-416e-a659-c76c518cb3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Позитивные/полупозитивные примеры в val_period: (53812, 3)\n",
      "        user_id  book_id  rel\n",
      "215249  1551451  2573361    2\n",
      "215250  1397150  2538344    2\n",
      "215251  1358090  2019613    2\n",
      "215252   849910  2366271    2\n",
      "215253   849910  1716389    1\n",
      "Количество холодных кандидатов: (60495, 3)\n",
      "   user_id  book_id  rel\n",
      "0  1551451   459282    0\n",
      "1  1551451  2287749    0\n",
      "2  1551451  2318816    0\n",
      "3  1551451  1796985    0\n",
      "4  1551451  1360858    0\n",
      "Итоговый размер val_candidates: (113171, 3)\n",
      "   user_id  book_id  rel\n",
      "0  1551451  2573361    2\n",
      "1  1397150  2538344    2\n",
      "2  1358090  2019613    2\n",
      "3   849910  2366271    2\n",
      "4   849910  1716389    1\n"
     ]
    }
   ],
   "source": [
    "val_period = val_period.copy()\n",
    "val_period['rel'] = np.where(val_period['has_read'] == 1, 2, 1)\n",
    "val_pos = val_period[['user_id', 'book_id', 'rel']].drop_duplicates()\n",
    "\n",
    "print(\"Позитивные/полупозитивные примеры в val_period:\", val_pos.shape)\n",
    "print(val_pos.head())\n",
    "\n",
    "user_hist_books_hist, popular_books_hist = build_history_and_popularity(train_hist)\n",
    "val_users = val_period['user_id'].unique()\n",
    "val_cold = build_cold_candidates(val_users, user_hist_books_hist, popular_books_hist, n_cold=N_COLD)\n",
    "\n",
    "print(\"Количество холодных кандидатов:\", val_cold.shape)\n",
    "print(val_cold.head())\n",
    "\n",
    "val_candidates = pd.concat([val_pos, val_cold], ignore_index=True)\n",
    "val_candidates = val_candidates.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Итоговый размер val_candidates:\", val_candidates.shape)\n",
    "print(val_candidates.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1661778-dfed-4722-becd-5773f598c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train_candidates_hist: (305519, 3)\n",
      "Распределение rel в train_candidates_hist:\n",
      "rel\n",
      "0     90270\n",
      "1     89214\n",
      "2    126035\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_hist_full = train_hist.copy()\n",
    "train_hist_full['rel'] = np.where(train_hist_full['has_read'] == 1, 2, 1)\n",
    "train_hist_pos = train_hist_full[['user_id', 'book_id', 'rel']].drop_duplicates()\n",
    "\n",
    "train_hist_users = train_hist_full['user_id'].unique()\n",
    "train_hist_cold = build_cold_candidates(train_hist_users, user_hist_books_hist, popular_books_hist, n_cold=N_COLD)\n",
    "\n",
    "train_candidates_hist = pd.concat([train_hist_pos, train_hist_cold], ignore_index=True)\n",
    "train_candidates_hist = train_candidates_hist.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Размер train_candidates_hist:\", train_candidates_hist.shape)\n",
    "print(\"Распределение rel в train_candidates_hist:\")\n",
    "print(train_candidates_hist['rel'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52156cf3-77c0-4784-834e-63f173000483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример агрегатов по книгам (hist):\n",
      "   book_id  n_interactions  n_read  n_plan  read_rate  plan_rate\n",
      "0       20             111      94      17   0.846847   0.153153\n",
      "1       35               1       1       0   0.999999   0.000000\n",
      "2       52               1       1       0   0.999999   0.000000\n",
      "3       54               5       4       1   0.800000   0.200000\n",
      "4       69               1       1       0   0.999999   0.000000\n",
      "Пример агрегатов по пользователям (hist):\n",
      "   user_id  u_n_interactions  u_n_read  u_n_plan  u_read_share\n",
      "0      151                75        36        39      0.480000\n",
      "1      210                31         0        31      0.000000\n",
      "2      560                 5         0         5      0.000000\n",
      "3     1380                46        19        27      0.413043\n",
      "4     1850                77        38        39      0.493506\n"
     ]
    }
   ],
   "source": [
    "book_stats_hist = (train_hist\n",
    "    .groupby('book_id')\n",
    "    .agg(\n",
    "        n_interactions=('user_id', 'nunique'),\n",
    "        n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "book_stats_hist['read_rate'] = book_stats_hist['n_read'] / (book_stats_hist['n_read'] + book_stats_hist['n_plan'] + 1e-6)\n",
    "book_stats_hist['plan_rate'] = book_stats_hist['n_plan'] / (book_stats_hist['n_read'] + book_stats_hist['n_plan'] + 1e-6)\n",
    "\n",
    "user_stats_hist = (train_hist\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        u_n_interactions=('book_id', 'nunique'),\n",
    "        u_n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        u_n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_stats_hist['u_read_share'] = user_stats_hist['u_n_read'] / (user_stats_hist['u_n_interactions'] + 1e-6)\n",
    "\n",
    "print(\"Пример агрегатов по книгам (hist):\")\n",
    "print(book_stats_hist.head())\n",
    "print(\"Пример агрегатов по пользователям (hist):\")\n",
    "print(user_stats_hist.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6528a2-dc88-4187-b7f5-1ce85df2898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(df_candidates, train_hist_df):\n",
    "    df = df_candidates.copy()\n",
    "    interactions_set = set(train_hist_df.set_index(['user_id', 'book_id']).index)\n",
    "    df['has_interacted'] = df.apply(lambda row: (row['user_id'], row['book_id']) in interactions_set, axis=1).astype(int)\n",
    "    user_book_rating = train_hist_df.groupby(['user_id', 'book_id'])['rating'].first().to_dict()\n",
    "    df['user_book_rating'] = df.apply(lambda row: user_book_rating.get((row['user_id'], row['book_id']), 0), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_temporal_features(df_candidates, train_hist_df, split_time=None):\n",
    "    df = df_candidates.copy()\n",
    "    user_read_timestamps = (\n",
    "        train_hist_df[train_hist_df['has_read'] == 1]\n",
    "        .groupby('user_id')['timestamp']\n",
    "        .agg(['mean', 'max', 'min'])\n",
    "        .add_prefix('u_read_timestamp_')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(user_read_timestamps, on='user_id', how='left')\n",
    "    book_timestamps = (\n",
    "        train_hist_df.groupby('book_id')['timestamp']\n",
    "        .agg(['mean', 'max', 'min'])\n",
    "        .add_prefix('b_timestamp_')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(book_timestamps, on='book_id', how='left')\n",
    "    base_date = pd.Timestamp('1970-01-01')\n",
    "    temporal_cols_to_convert = [\n",
    "        'u_read_timestamp_mean', 'u_read_timestamp_max', 'u_read_timestamp_min',\n",
    "        'b_timestamp_mean', 'b_timestamp_max', 'b_timestamp_min'\n",
    "    ]\n",
    "    for col in temporal_cols_to_convert:\n",
    "        if col in df.columns and pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = (df[col] - base_date).dt.days\n",
    "    if split_time is None:\n",
    "        prediction_time_days = (train_hist_df['timestamp'].max() - base_date).days\n",
    "    else:\n",
    "        prediction_time_days = (split_time - base_date).days\n",
    "    df['days_since_last_interaction'] = (prediction_time_days - df['b_timestamp_max']).fillna(10000)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_collaborative_features(df_candidates, train_hist_df, n_factors=16):\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    df = df_candidates.copy()\n",
    "    interaction_matrix = train_hist_df.pivot(index='user_id', columns='book_id', values='has_read').fillna(0)\n",
    "    svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "    user_factors = svd.fit_transform(interaction_matrix)\n",
    "    item_factors = svd.components_.T\n",
    "    user_factors_dict = {uid: vec for uid, vec in zip(interaction_matrix.index, user_factors)}\n",
    "    item_factors_dict = {bid: vec for bid, vec in zip(interaction_matrix.columns, item_factors)}\n",
    "    def get_user_factor_features(user_id):\n",
    "        return user_factors_dict.get(user_id, np.zeros(n_factors))\n",
    "    def get_book_factor_features(book_id):\n",
    "        return item_factors_dict.get(book_id, np.zeros(n_factors))\n",
    "    user_factor_features = df['user_id'].apply(get_user_factor_features).apply(pd.Series)\n",
    "    user_factor_features.columns = [f'user_factor_{i}' for i in range(n_factors)]\n",
    "    book_factor_features = df['book_id'].apply(get_book_factor_features).apply(pd.Series)\n",
    "    book_factor_features.columns = [f'book_factor_{i}' for i in range(n_factors)]\n",
    "    df = pd.concat([df, user_factor_features, book_factor_features], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_popularity_trend_features(df_candidates, train_hist_df):\n",
    "    df = df_candidates.copy()\n",
    "    train_hist_with_year = train_hist_df.copy()\n",
    "    train_hist_with_year['year'] = train_hist_with_year['timestamp'].dt.year\n",
    "    last_year = train_hist_with_year['year'].max()\n",
    "    book_popularity_last_year = (\n",
    "        train_hist_with_year[train_hist_with_year['year'] == last_year]\n",
    "        .groupby('book_id')['user_id']\n",
    "        .nunique()\n",
    "        .rename('book_popularity_last_year')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(book_popularity_last_year, on='book_id', how='left')\n",
    "    df['book_popularity_last_year'] = df['book_popularity_last_year'].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_genre_features(df_candidates, train_hist_df, books_metadata_df):\n",
    "    df = df_candidates.copy()\n",
    "    df = df.merge(books_metadata_df[['book_id', 'genre']], on='book_id', how='left')\n",
    "    user_preferred_genres = (\n",
    "        train_hist_df.merge(books_metadata_df[['book_id', 'genre']], on='book_id', how='left')\n",
    "        .groupby(['user_id', 'genre'])['book_id']\n",
    "        .count()\n",
    "        .groupby('user_id')\n",
    "        .idxmax()\n",
    "        .apply(lambda x: x[1])\n",
    "        .rename('preferred_genre')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(user_preferred_genres, on='user_id', how='left')\n",
    "    df['genre_match'] = (df['genre'] == df['preferred_genre']).astype(int)\n",
    "    df['genre_match'] = df['genre_match'].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_all_new_features(df_candidates, train_hist_df, books_metadata_df=None, split_time=None):\n",
    "    df = df_candidates.copy()\n",
    "    df = add_interaction_features(df, train_hist_df)\n",
    "    df = add_temporal_features(df, train_hist_df, split_time=split_time)\n",
    "    df = add_popularity_trend_features(df, train_hist_df)\n",
    "    df = add_collaborative_features(df, train_hist_df, n_factors=16)\n",
    "    if books_metadata_df is not None:\n",
    "        df = add_genre_features(df, train_hist_df, books_metadata_df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05ed0b02-e1fc-49f7-b68e-46baf40f171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 54\n",
      "Распределение rel в train_features_hist:\n",
      "rel\n",
      "0     90270\n",
      "1     89220\n",
      "2    126043\n",
      "Name: count, dtype: int64\n",
      "Распределение rel в val_features:\n",
      "rel\n",
      "0    59359\n",
      "1    23246\n",
      "2    30569\n",
      "Name: count, dtype: int64\n",
      "pos_weight_read: 1.4240378283488648\n",
      "pos_weight_any: 0.41934749585196085\n"
     ]
    }
   ],
   "source": [
    "train_features_hist = add_basic_features(train_candidates_hist, user_stats_hist, book_stats_hist)\n",
    "val_features = add_basic_features(val_candidates, user_stats_hist, book_stats_hist)\n",
    "\n",
    "train_features_hist = add_all_new_features(train_features_hist, train_hist_full, books_metadata_df=books_metadata_df, split_time=split_point)\n",
    "val_features = add_all_new_features(val_features, train_hist_full, books_metadata_df=books_metadata_df, split_time=split_point)\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in train_features_hist.columns\n",
    "    if c not in ['user_id', 'book_id', 'rel']\n",
    "]\n",
    "\n",
    "X_train = train_features_hist[feature_cols]\n",
    "y_train_rel = train_features_hist['rel']\n",
    "y_train_read = (train_features_hist['rel'] == 2).astype(int)\n",
    "y_train_any = (train_features_hist['rel'] > 0).astype(int)\n",
    "\n",
    "X_val = val_features[feature_cols]\n",
    "y_val_rel = val_features['rel']\n",
    "y_val_read = (val_features['rel'] == 2).astype(int)\n",
    "y_val_any = (val_features['rel'] > 0).astype(int)\n",
    "\n",
    "pos_weight_read = (len(y_train_read) - y_train_read.sum()) / (y_train_read.sum() + 1e-6)\n",
    "pos_weight_any = (len(y_train_any) - y_train_any.sum()) / (y_train_any.sum() + 1e-6)\n",
    "\n",
    "print(\"Число признаков:\", len(feature_cols))\n",
    "print(\"Распределение rel в train_features_hist:\")\n",
    "print(y_train_rel.value_counts().sort_index())\n",
    "print(\"Распределение rel в val_features:\")\n",
    "print(y_val_rel.value_counts().sort_index())\n",
    "print(\"pos_weight_read:\", pos_weight_read)\n",
    "print(\"pos_weight_any:\", pos_weight_any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8fdcc6d-c583-4bce-a483-3e0e22436070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример score_rank на валидации:\n",
      "   user_id  book_id  rel  score_rank\n",
      "0  1551451  2573361    2   -2.480797\n",
      "1  1397150  2538344    2   -2.418944\n",
      "2  1358090  2019613    2   -4.729824\n",
      "3   849910  2366271    2   -5.287802\n",
      "4   849910  1716389    1   -7.039556\n"
     ]
    }
   ],
   "source": [
    "xgb_read_cv = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=float(pos_weight_read)\n",
    ")\n",
    "\n",
    "xgb_any_cv = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=43,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=float(pos_weight_any)\n",
    ")\n",
    "\n",
    "xgb_read_cv.fit(X_train, y_train_read)\n",
    "xgb_any_cv.fit(X_train, y_train_any)\n",
    "\n",
    "cb_read_cv = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=42,\n",
    "    class_weights=[1.0, float(pos_weight_read)]\n",
    ")\n",
    "\n",
    "cb_any_cv = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=43,\n",
    "    class_weights=[1.0, float(pos_weight_any)]\n",
    ")\n",
    "\n",
    "cb_read_cv.fit(X_train, y_train_read)\n",
    "cb_any_cv.fit(X_train, y_train_any)\n",
    "\n",
    "p_read_xgb_val = xgb_read_cv.predict_proba(X_val)[:, 1]\n",
    "p_any_xgb_val = xgb_any_cv.predict_proba(X_val)[:, 1]\n",
    "\n",
    "p_read_cb_val = cb_read_cv.predict_proba(X_val)[:, 1]\n",
    "p_any_cb_val = cb_any_cv.predict_proba(X_val)[:, 1]\n",
    "\n",
    "train_rank = train_features_hist[['user_id', 'book_id', 'rel'] + feature_cols].copy()\n",
    "train_rank = train_rank.sort_values('user_id').reset_index(drop=True)\n",
    "\n",
    "X_train_rank = train_rank[feature_cols]\n",
    "y_train_rank = train_rank['rel']\n",
    "group_id_train = train_rank['user_id']\n",
    "\n",
    "cb_rank_cv = CatBoostRanker(\n",
    "    iterations=700,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    loss_function='YetiRank',\n",
    "    verbose=False,\n",
    "    random_seed=123\n",
    ")\n",
    "\n",
    "cb_rank_cv.fit(\n",
    "    X_train_rank,\n",
    "    y_train_rank,\n",
    "    group_id=group_id_train\n",
    ")\n",
    "\n",
    "score_rank_val = cb_rank_cv.predict(X_val)\n",
    "val_features['score_rank'] = score_rank_val\n",
    "\n",
    "print(\"Пример score_rank на валидации:\")\n",
    "print(val_features[['user_id', 'book_id', 'rel', 'score_rank']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "013b7cb5-ddda-4ecd-b587-acbf51b96686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры ансамбля по валидации:\n",
      "w1 (read) = 0.8 w2 (any) = 0.19999999999999996\n",
      "alpha (XGB vs CB) = 0.6\n",
      "beta (ranker) = 0.0\n",
      "Лучший NDCG@20 на валидации: 0.9490246757103888\n",
      "Проверка NDCG@20 с лучшими параметрами: 0.9490246757103888\n"
     ]
    }
   ],
   "source": [
    "w1_grid = [0.6, 0.7, 0.8]\n",
    "alpha_grid = [0.4, 0.5, 0.6]\n",
    "beta_grid = [0.0, 0.2, 0.4]\n",
    "\n",
    "best_ndcg = -1.0\n",
    "best_w1 = None\n",
    "best_w2 = None\n",
    "best_alpha = None\n",
    "best_beta = None\n",
    "\n",
    "for w1_candidate in w1_grid:\n",
    "    w2_candidate = 1.0 - w1_candidate\n",
    "    score_xgb_local = w1_candidate * p_read_xgb_val + w2_candidate * p_any_xgb_val\n",
    "    score_cb_local = w1_candidate * p_read_cb_val + w2_candidate * p_any_cb_val\n",
    "    for alpha_candidate in alpha_grid:\n",
    "        base_ens = alpha_candidate * score_xgb_local + (1.0 - alpha_candidate) * score_cb_local\n",
    "        for beta_candidate in beta_grid:\n",
    "            final_pred = (1.0 - beta_candidate) * base_ens + beta_candidate * score_rank_val\n",
    "            df_tmp = val_features[['user_id', 'book_id', 'rel']].copy()\n",
    "            df_tmp['pred'] = final_pred\n",
    "            ndcg_val = mean_ndcg(df_tmp, k=20)\n",
    "            if ndcg_val > best_ndcg:\n",
    "                best_ndcg = ndcg_val\n",
    "                best_w1 = w1_candidate\n",
    "                best_w2 = w2_candidate\n",
    "                best_alpha = alpha_candidate\n",
    "                best_beta = beta_candidate\n",
    "\n",
    "w1 = best_w1\n",
    "w2 = best_w2\n",
    "\n",
    "print(\"Лучшие параметры ансамбля по валидации:\")\n",
    "print(\"w1 (read) =\", best_w1, \"w2 (any) =\", best_w2)\n",
    "print(\"alpha (XGB vs CB) =\", best_alpha)\n",
    "print(\"beta (ranker) =\", best_beta)\n",
    "print(\"Лучший NDCG@20 на валидации:\", best_ndcg)\n",
    "\n",
    "score_xgb_best = w1 * p_read_xgb_val + w2 * p_any_xgb_val\n",
    "score_cb_best = w1 * p_read_cb_val + w2 * p_any_cb_val\n",
    "base_ens_best = best_alpha * score_xgb_best + (1.0 - best_alpha) * score_cb_best\n",
    "final_pred_best = (1.0 - best_beta) * base_ens_best + best_beta * score_rank_val\n",
    "\n",
    "df_val_best = val_features[['user_id', 'book_id', 'rel']].copy()\n",
    "df_val_best['pred'] = final_pred_best\n",
    "ndcg20_final = mean_ndcg(df_val_best, k=20)\n",
    "print(\"Проверка NDCG@20 с лучшими параметрами:\", ndcg20_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ce0159-9009-45a7-8e72-ee70a6296965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train_candidates_full: (378396, 3)\n",
      "Распределение rel в train_candidates_full:\n",
      "rel\n",
      "0    109335\n",
      "1    112458\n",
      "2    156603\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_full = train.copy()\n",
    "train_full['rel'] = np.where(train_full['has_read'] == 1, 2, 1)\n",
    "train_pos_full = train_full[['user_id', 'book_id', 'rel']].drop_duplicates()\n",
    "\n",
    "user_hist_books_full, popular_books_full = build_history_and_popularity(train)\n",
    "train_users_full = train_full['user_id'].unique()\n",
    "train_cold_full = build_cold_candidates(train_users_full, user_hist_books_full, popular_books_full, n_cold=N_COLD)\n",
    "\n",
    "train_candidates_full = pd.concat([train_pos_full, train_cold_full], ignore_index=True)\n",
    "train_candidates_full = train_candidates_full.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Размер train_candidates_full:\", train_candidates_full.shape)\n",
    "print(\"Распределение rel в train_candidates_full:\")\n",
    "print(train_candidates_full['rel'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfbdff1b-ed70-4f96-853e-d40d1018a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример агрегатов по книгам (full):\n",
      "   book_id  n_interactions  n_read  n_plan  read_rate  plan_rate\n",
      "0       20             122     103      19   0.844262   0.155738\n",
      "1       35               1       1       0   0.999999   0.000000\n",
      "2       52               1       1       0   0.999999   0.000000\n",
      "3       54               7       5       2   0.714286   0.285714\n",
      "4       69               1       1       0   0.999999   0.000000\n",
      "Пример агрегатов по пользователям (full):\n",
      "   user_id  u_n_interactions  u_n_read  u_n_plan  u_read_share\n",
      "0      151                75        36        39      0.480000\n",
      "1      210                31         0        31      0.000000\n",
      "2      560                 6         0         6      0.000000\n",
      "3     1380                56        29        27      0.517857\n",
      "4     1850                77        38        39      0.493506\n"
     ]
    }
   ],
   "source": [
    "book_stats_full = (train\n",
    "    .groupby('book_id')\n",
    "    .agg(\n",
    "        n_interactions=('user_id', 'nunique'),\n",
    "        n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "book_stats_full['read_rate'] = book_stats_full['n_read'] / (book_stats_full['n_read'] + book_stats_full['n_plan'] + 1e-6)\n",
    "book_stats_full['plan_rate'] = book_stats_full['n_plan'] / (book_stats_full['n_read'] + book_stats_full['n_plan'] + 1e-6)\n",
    "\n",
    "user_stats_full = (train\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        u_n_interactions=('book_id', 'nunique'),\n",
    "        u_n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        u_n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_stats_full['u_read_share'] = user_stats_full['u_n_read'] / (user_stats_full['u_n_interactions'] + 1e-6)\n",
    "\n",
    "print(\"Пример агрегатов по книгам (full):\")\n",
    "print(book_stats_full.head())\n",
    "print(\"Пример агрегатов по пользователям (full):\")\n",
    "print(user_stats_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30e166a5-c260-4816-8651-9f2fb3a7dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков (full): 54\n",
      "Распределение rel (full):\n",
      "rel\n",
      "0    109335\n",
      "1    112466\n",
      "2    156612\n",
      "Name: count, dtype: int64\n",
      "pos_weight_read_full: 1.416245243011926\n",
      "pos_weight_any_full: 0.4063319929522059\n"
     ]
    }
   ],
   "source": [
    "train_features_full = add_basic_features(train_candidates_full, user_stats_full, book_stats_full)\n",
    "train_features_full = add_all_new_features(train_features_full, train, books_metadata_df=books_metadata_df, split_time=train['timestamp'].max())\n",
    "\n",
    "feature_cols_full = [\n",
    "    c for c in train_features_full.columns\n",
    "    if c not in ['user_id', 'book_id', 'rel']\n",
    "]\n",
    "\n",
    "X_full = train_features_full[feature_cols_full]\n",
    "y_full_rel = train_features_full['rel']\n",
    "y_full_read = (train_features_full['rel'] == 2).astype(int)\n",
    "y_full_any = (train_features_full['rel'] > 0).astype(int)\n",
    "\n",
    "pos_weight_read_full = (len(y_full_read) - y_full_read.sum()) / (y_full_read.sum() + 1e-6)\n",
    "pos_weight_any_full = (len(y_full_any) - y_full_any.sum()) / (y_full_any.sum() + 1e-6)\n",
    "\n",
    "print(\"Число признаков (full):\", len(feature_cols_full))\n",
    "print(\"Распределение rel (full):\")\n",
    "print(y_full_rel.value_counts().sort_index())\n",
    "print(\"pos_weight_read_full:\", pos_weight_read_full)\n",
    "print(\"pos_weight_any_full:\", pos_weight_any_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ca8666-56fd-42b6-b625-b7c7c1e1b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальные модели (XGBoost + CatBoost + CatBoostRanker) обучены на полном тренировочном датасете.\n"
     ]
    }
   ],
   "source": [
    "xgb_read_full = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=100,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=float(pos_weight_read_full)\n",
    ")\n",
    "\n",
    "xgb_any_full = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=101,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=float(pos_weight_any_full)\n",
    ")\n",
    "\n",
    "xgb_read_full.fit(X_full, y_full_read)\n",
    "xgb_any_full.fit(X_full, y_full_any)\n",
    "\n",
    "cb_read_full = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=100,\n",
    "    class_weights=[1.0, float(pos_weight_read_full)]\n",
    ")\n",
    "\n",
    "cb_any_full = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=101,\n",
    "    class_weights=[1.0, float(pos_weight_any_full)]\n",
    ")\n",
    "\n",
    "cb_read_full.fit(X_full, y_full_read)\n",
    "cb_any_full.fit(X_full, y_full_any)\n",
    "\n",
    "train_rank_full = train_features_full[['user_id', 'book_id', 'rel'] + feature_cols_full].copy()\n",
    "train_rank_full = train_rank_full.sort_values('user_id').reset_index(drop=True)\n",
    "\n",
    "X_full_rank = train_rank_full[feature_cols_full]\n",
    "y_full_rank = train_rank_full['rel']\n",
    "group_id_full = train_rank_full['user_id']\n",
    "\n",
    "cb_rank_full = CatBoostRanker(\n",
    "    iterations=700,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    loss_function='YetiRank',\n",
    "    verbose=False,\n",
    "    random_seed=200\n",
    ")\n",
    "\n",
    "cb_rank_full.fit(\n",
    "    X_full_rank,\n",
    "    y_full_rank,\n",
    "    group_id=group_id_full\n",
    ")\n",
    "\n",
    "print(\"Финальные модели (XGBoost + CatBoost + CatBoostRanker) обучены на полном тренировочном датасете.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fabe067-0e0f-4073-ba86-675a55c692db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер candidates_raw: (3512, 2)\n",
      "Колонки candidates_raw: ['user_id', 'book_id_list']\n",
      "   user_id                                       book_id_list\n",
      "0      210  11936,254097,709075,840500,971259,1037723,1074...\n",
      "1     1380  8369,28302,145975,482934,625734,998313,1098150...\n",
      "2     2050  4902,8369,18790,308364,317849,460492,822326,86...\n",
      "3     2740  39221,112023,149611,162418,181062,317050,43565...\n",
      "4     4621  28638,28639,28642,28901,31479,307058,475353,57...\n",
      "Длинный формат candidates_long: (81048, 2)\n",
      "   user_id  book_id\n",
      "0      210    11936\n",
      "0      210   254097\n",
      "0      210   709075\n",
      "0      210   840500\n",
      "0      210   971259\n"
     ]
    }
   ],
   "source": [
    "candidates_path = \"./data/raw/candidates.csv\"\n",
    "candidates_raw = pd.read_csv(candidates_path)\n",
    "\n",
    "print(\"Размер candidates_raw:\", candidates_raw.shape)\n",
    "print(\"Колонки candidates_raw:\", candidates_raw.columns.tolist())\n",
    "print(candidates_raw.head())\n",
    "\n",
    "candidates_long = candidates_raw.copy()\n",
    "candidates_long['book_id_list'] = candidates_long['book_id_list'].fillna('').astype(str)\n",
    "candidates_long['book_id_list'] = candidates_long['book_id_list'].str.split(',')\n",
    "candidates_long = candidates_long.explode('book_id_list')\n",
    "candidates_long = candidates_long[candidates_long['book_id_list'].str.strip() != '']\n",
    "candidates_long['book_id'] = candidates_long['book_id_list'].str.strip().astype(int)\n",
    "candidates_long = candidates_long[['user_id', 'book_id']].drop_duplicates()\n",
    "\n",
    "print(\"Длинный формат candidates_long:\", candidates_long.shape)\n",
    "print(candidates_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "314aa949-7b1b-49e4-90ca-9a7e62ed4220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер test_features после join'ов: (81048, 56)\n",
      "   user_id  book_id  u_n_interactions  u_n_read  u_n_plan  u_read_share  \\\n",
      "0      210    11936                31         0        31           0.0   \n",
      "1      210   254097                31         0        31           0.0   \n",
      "2      210   709075                31         0        31           0.0   \n",
      "3      210   840500                31         0        31           0.0   \n",
      "4      210   971259                31         0        31           0.0   \n",
      "\n",
      "   n_interactions  n_read  n_plan  read_rate  ...  book_factor_9  \\\n",
      "0           396.0   375.0    21.0   0.946970  ...      -0.020593   \n",
      "1           360.0   325.0    35.0   0.902778  ...      -0.021495   \n",
      "2           198.0   130.0    68.0   0.656566  ...       0.009586   \n",
      "3            91.0    70.0    21.0   0.769231  ...       0.097108   \n",
      "4             1.0     0.0     1.0   0.000000  ...       0.000000   \n",
      "\n",
      "   book_factor_10  book_factor_11  book_factor_12  book_factor_13  \\\n",
      "0        0.005309       -0.020688       -0.002351       -0.020465   \n",
      "1       -0.006961       -0.035004       -0.014883       -0.024500   \n",
      "2        0.024831       -0.019035        0.031264       -0.010225   \n",
      "3        0.011294        0.041625       -0.130131        0.144289   \n",
      "4        0.000000       -0.000000       -0.000000       -0.000000   \n",
      "\n",
      "   book_factor_14  book_factor_15  genre  preferred_genre  genre_match  \n",
      "0       -0.022659        0.028319    146              149            0  \n",
      "1       -0.002930        0.009107    426              149            0  \n",
      "2       -0.024776       -0.038472    127              149            0  \n",
      "3        0.036450        0.029405    149              149            1  \n",
      "4       -0.000000        0.000000    149              149            1  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "Пример предсказаний на candidates_long:\n",
      "   user_id  book_id      pred\n",
      "0      210    11936  0.000056\n",
      "1      210   254097  0.000125\n",
      "2      210   709075  0.000203\n",
      "3      210   840500  0.006484\n",
      "4      210   971259  0.034929\n"
     ]
    }
   ],
   "source": [
    "test_features = add_basic_features(candidates_long, user_stats_full, book_stats_full)\n",
    "test_features = add_all_new_features(test_features, train, books_metadata_df=books_metadata_df, split_time=train['timestamp'].max())\n",
    "\n",
    "test_features = test_features.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Размер test_features после join'ов:\", test_features.shape)\n",
    "print(test_features.head())\n",
    "\n",
    "X_test = test_features[feature_cols_full]\n",
    "\n",
    "\n",
    "p_read_xgb_test = xgb_read_full.predict_proba(X_test)[:, 1]\n",
    "p_any_xgb_test = xgb_any_full.predict_proba(X_test)[:, 1]\n",
    "score_xgb_test = w1 * p_read_xgb_test + w2 * p_any_xgb_test\n",
    "\n",
    "p_read_cb_test = cb_read_full.predict_proba(X_test)[:, 1]\n",
    "p_any_cb_test = cb_any_full.predict_proba(X_test)[:, 1]\n",
    "score_cb_test = w1 * p_read_cb_test + w2 * p_any_cb_test\n",
    "\n",
    "score_rank_test = cb_rank_full.predict(X_test)\n",
    "\n",
    "base_ens_test = best_alpha * score_xgb_test + (1.0 - best_alpha) * score_cb_test\n",
    "final_pred_test = (1.0 - best_beta) * base_ens_test + best_beta * score_rank_test\n",
    "\n",
    "test_features['pred'] = final_pred_test\n",
    "\n",
    "print(\"Пример предсказаний на candidates_long:\")\n",
    "print(test_features[['user_id', 'book_id', 'pred']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38b3672c-e86f-4000-8305-e92a5b9b961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример сабмита (формат A):\n",
      "   user_id                                       book_id_list\n",
      "0      210  1281035,971259,2447113,1673950,3015694,3988468...\n",
      "1     1380  2548861,2290484,482934,1326209,2379664,2351675...\n",
      "2     2050  460492,1021078,2053462,2254200,317849,822326,2...\n",
      "3     2740  549194,1834192,2307893,2479424,5535190,987516,...\n",
      "4     4621  4841518,3185149,4472386,2347568,4294558,633703...\n",
      "Сабмит сохранён в: ./output/submissions/submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission_user_list = make_submission_user_list(test_features, top_k=20)\n",
    "\n",
    "print(\"Пример сабмита (формат A):\")\n",
    "print(submission_user_list.head())\n",
    "\n",
    "submit_path = \"./output/submissions/submission.csv\"\n",
    "submission_user_list.to_csv(submit_path, index=False)\n",
    "print(\"Сабмит сохранён в:\", submit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85cd8b-7fbb-4167-96a7-4ab298ad4698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d0128-3d66-401a-b915-95c957e4c157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
