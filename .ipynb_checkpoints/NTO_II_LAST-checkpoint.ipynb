{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda40e2f-a1b3-4eb9-b2f5-3e4feb275426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from math import log2\n",
    "\n",
    "w1 = 0.7\n",
    "w2 = 0.3\n",
    "N_COLD = 15\n",
    "SPLIT_Q = 0.8\n",
    "train_path = \"./data/raw/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66c75fa-0860-44a2-8df3-ce595e29f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id               int64\n",
      "book_id               int64\n",
      "has_read              int64\n",
      "rating                int64\n",
      "timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "   user_id  book_id  has_read  rating           timestamp\n",
      "0     3870   310170         0       0 2008-04-27 21:06:16\n",
      "1     3870   306406         0       0 2008-06-07 11:51:01\n",
      "2     4091   195676         0       0 2008-08-06 00:40:55\n",
      "3     3870   554261         1       8 2008-08-07 09:16:12\n",
      "4     3870    33078         1       2 2008-08-07 09:17:20\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "\n",
    "print(train.dtypes.head())\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0923b8e4-0d4f-4e3f-8c77-78988416995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_split (80-й перцентиль): 2020-09-11 23:28:35\n",
      "Размер train_hist: (215249, 5)\n",
      "Размер val_period: (53812, 5)\n",
      "\n",
      "train_hist диапазон времени: 2008-04-27 21:06:16 → 2020-09-11 23:28:35\n",
      "val_period диапазон времени: 2020-09-11 23:30:30 → 2021-09-06 00:17:11\n"
     ]
    }
   ],
   "source": [
    "split_point = train['timestamp'].quantile(SPLIT_Q)\n",
    "print(\"T_split (80-й перцентиль):\", split_point)\n",
    "\n",
    "train_hist = train[train['timestamp'] <= split_point].copy()\n",
    "val_period = train[train['timestamp'] > split_point].copy()\n",
    "\n",
    "print(\"Размер train_hist:\", train_hist.shape)\n",
    "print(\"Размер val_period:\", val_period.shape)\n",
    "\n",
    "print(\"\\ntrain_hist диапазон времени:\",\n",
    "      train_hist['timestamp'].min(), \"→\", train_hist['timestamp'].max())\n",
    "print(\"val_period диапазон времени:\",\n",
    "      val_period['timestamp'].min(), \"→\", val_period['timestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93db8a9-a190-4029-8bcc-f148fadb0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_history_and_popularity(df):\n",
    "    user_hist_books = (\n",
    "        df\n",
    "        .groupby('user_id')['book_id']\n",
    "        .agg(lambda x: set(x.tolist()))\n",
    "        .to_dict()\n",
    "    )\n",
    "    book_popularity = (\n",
    "        df\n",
    "        .groupby('book_id')['user_id']\n",
    "        .nunique()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    popular_books = book_popularity.index.to_numpy()\n",
    "    return user_hist_books, popular_books\n",
    "\n",
    "\n",
    "def sample_cold_candidates_for_user(user_id, user_hist_books, popular_books, n_cold=N_COLD):\n",
    "    seen = user_hist_books.get(user_id, set())\n",
    "    cold = []\n",
    "    for b in popular_books:\n",
    "        if b not in seen:\n",
    "            cold.append(b)\n",
    "            if len(cold) >= n_cold:\n",
    "                break\n",
    "    return cold\n",
    "\n",
    "\n",
    "def build_cold_candidates(users, user_hist_books, popular_books, n_cold=N_COLD):\n",
    "    rows = []\n",
    "    for u in users:\n",
    "        cold_books = sample_cold_candidates_for_user(u, user_hist_books, popular_books, n_cold=n_cold)\n",
    "        for b in cold_books:\n",
    "            rows.append((u, b, 0))\n",
    "    df = pd.DataFrame(rows, columns=['user_id', 'book_id', 'rel'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_basic_features(df_candidates, user_stats_df, book_stats_df):\n",
    "    df = df_candidates.copy()\n",
    "    df = df.merge(user_stats_df, on='user_id', how='left')\n",
    "    df = df.merge(book_stats_df, on='book_id', how='left')\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c60b885-0745-405b-9168-09f3861c0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(rels, k=20):\n",
    "    rels = np.asarray(rels)[:k]\n",
    "    if rels.size == 0:\n",
    "        return 0.0\n",
    "    return float(sum(rel / log2(i + 2) for i, rel in enumerate(rels)))\n",
    "\n",
    "\n",
    "def ndcg_for_user(df_u, k=20):\n",
    "    df_sorted = df_u.sort_values('pred', ascending=False)\n",
    "    rels_pred = df_sorted['rel'].values\n",
    "    dcg = dcg_at_k(rels_pred, k=k)\n",
    "    ideal_rels = np.sort(df_u['rel'].values)[::-1]\n",
    "    idcg = dcg_at_k(ideal_rels, k=k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def mean_ndcg(df, k=20):\n",
    "    scores = []\n",
    "    for user_id, df_u in df.groupby('user_id'):\n",
    "        score_u = ndcg_for_user(df_u, k=k)\n",
    "        scores.append(score_u)\n",
    "    if not scores:\n",
    "        return 0.0\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def make_submission_user_list(df_pred, top_k=20):\n",
    "    submission_rows = []\n",
    "    for user_id, df_u in df_pred.groupby('user_id'):\n",
    "        df_sorted = df_u.sort_values('pred', ascending=False)\n",
    "        top_books = df_sorted['book_id'].head(top_k).tolist()\n",
    "        book_id_list_str = \",\".join(map(str, top_books))\n",
    "        submission_rows.append((user_id, book_id_list_str))\n",
    "    sub = pd.DataFrame(submission_rows, columns=['user_id', 'book_id_list'])\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced047e2-3577-43b0-9d16-1f4ed96c2081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Позитивные/полупозитивные примеры в val_period: (53812, 3)\n",
      "        user_id  book_id  rel\n",
      "215249  1551451  2573361    2\n",
      "215250  1397150  2538344    2\n",
      "215251  1358090  2019613    2\n",
      "215252   849910  2366271    2\n",
      "215253   849910  1716389    1\n",
      "Количество холодных кандидатов: (60495, 3)\n",
      "   user_id  book_id  rel\n",
      "0  1551451   459282    0\n",
      "1  1551451  2287749    0\n",
      "2  1551451  2318816    0\n",
      "3  1551451  1796985    0\n",
      "4  1551451  1360858    0\n",
      "Итоговый размер val_candidates: (113171, 3)\n",
      "   user_id  book_id  rel\n",
      "0  1551451  2573361    2\n",
      "1  1397150  2538344    2\n",
      "2  1358090  2019613    2\n",
      "3   849910  2366271    2\n",
      "4   849910  1716389    1\n"
     ]
    }
   ],
   "source": [
    "val_period = val_period.copy()\n",
    "val_period['rel'] = np.where(val_period['has_read'] == 1, 2, 1)\n",
    "val_pos = val_period[['user_id', 'book_id', 'rel']].drop_duplicates()\n",
    "\n",
    "print(\"Позитивные/полупозитивные примеры в val_period:\", val_pos.shape)\n",
    "print(val_pos.head())\n",
    "\n",
    "user_hist_books_hist, popular_books_hist = build_history_and_popularity(train_hist)\n",
    "val_users = val_period['user_id'].unique()\n",
    "val_cold = build_cold_candidates(val_users, user_hist_books_hist, popular_books_hist, n_cold=N_COLD)\n",
    "\n",
    "print(\"Количество холодных кандидатов:\", val_cold.shape)\n",
    "print(val_cold.head())\n",
    "\n",
    "val_candidates = pd.concat([val_pos, val_cold], ignore_index=True)\n",
    "val_candidates = val_candidates.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Итоговый размер val_candidates:\", val_candidates.shape)\n",
    "print(val_candidates.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9971a5f-3c66-4141-9bad-362cceeaf11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train_candidates_hist: (305519, 3)\n",
      "Распределение rel в train_candidates_hist:\n",
      "rel\n",
      "0     90270\n",
      "1     89214\n",
      "2    126035\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_hist_full = train_hist.copy()\n",
    "train_hist_full['rel'] = np.where(train_hist_full['has_read'] == 1, 2, 1)\n",
    "train_hist_pos = train_hist_full[['user_id', 'book_id', 'rel']].drop_duplicates()\n",
    "\n",
    "train_hist_users = train_hist_full['user_id'].unique()\n",
    "train_hist_cold = build_cold_candidates(train_hist_users, user_hist_books_hist, popular_books_hist, n_cold=N_COLD)\n",
    "\n",
    "train_candidates_hist = pd.concat([train_hist_pos, train_hist_cold], ignore_index=True)\n",
    "train_candidates_hist = train_candidates_hist.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Размер train_candidates_hist:\", train_candidates_hist.shape)\n",
    "print(\"Распределение rel в train_candidates_hist:\")\n",
    "print(train_candidates_hist['rel'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894ea982-ccab-47de-aeac-1e898e29444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример агрегатов по книгам (hist):\n",
      "   book_id  n_interactions  n_read  n_plan  read_rate  plan_rate\n",
      "0       20             111      94      17   0.846847   0.153153\n",
      "1       35               1       1       0   0.999999   0.000000\n",
      "2       52               1       1       0   0.999999   0.000000\n",
      "3       54               5       4       1   0.800000   0.200000\n",
      "4       69               1       1       0   0.999999   0.000000\n",
      "Пример агрегатов по пользователям (hist):\n",
      "   user_id  u_n_interactions  u_n_read  u_n_plan  u_read_share\n",
      "0      151                75        36        39      0.480000\n",
      "1      210                31         0        31      0.000000\n",
      "2      560                 5         0         5      0.000000\n",
      "3     1380                46        19        27      0.413043\n",
      "4     1850                77        38        39      0.493506\n"
     ]
    }
   ],
   "source": [
    "book_stats_hist = (train_hist\n",
    "    .groupby('book_id')\n",
    "    .agg(\n",
    "        n_interactions=('user_id', 'nunique'),\n",
    "        n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "book_stats_hist['read_rate'] = book_stats_hist['n_read'] / (book_stats_hist['n_read'] + book_stats_hist['n_plan'] + 1e-6)\n",
    "book_stats_hist['plan_rate'] = book_stats_hist['n_plan'] / (book_stats_hist['n_read'] + book_stats_hist['n_plan'] + 1e-6)\n",
    "\n",
    "user_stats_hist = (train_hist\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        u_n_interactions=('book_id', 'nunique'),\n",
    "        u_n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        u_n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_stats_hist['u_read_share'] = user_stats_hist['u_n_read'] / (user_stats_hist['u_n_interactions'] + 1e-6)\n",
    "\n",
    "print(\"Пример агрегатов по книгам (hist):\")\n",
    "print(book_stats_hist.head())\n",
    "print(\"Пример агрегатов по пользователям (hist):\")\n",
    "print(user_stats_hist.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e6f68a-5af3-4783-b751-7ef4975c58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(df_candidates, train_hist_df):\n",
    "    \"\"\"\n",
    "    Добавляет признаки, связанные с историей взаимодействия пользователя с книгой.\n",
    "    \"\"\"\n",
    "    df = df_candidates.copy()\n",
    "    \n",
    "    # Признак: было ли взаимодействие (user_id, book_id) в train_hist\n",
    "    interactions_set = set(train_hist_df.set_index(['user_id', 'book_id']).index)\n",
    "    df['has_interacted'] = df.apply(lambda row: (row['user_id'], row['book_id']) in interactions_set, axis=1).astype(int)\n",
    "    \n",
    "    # Признак: рейтинг, который пользователь поставил этой книге (если ставил)\n",
    "    user_book_rating = train_hist_df.groupby(['user_id', 'book_id'])['rating'].first().to_dict()\n",
    "    df['user_book_rating'] = df.apply(lambda row: user_book_rating.get((row['user_id'], row['book_id']), 0), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_temporal_features(df_candidates, train_hist_df, split_time=None):\n",
    "    \"\"\"\n",
    "    Добавляет признаки, основанные на временных метках.\n",
    "    split_time: datetime - время разбиения. Используется для вычисления days_since_last_interaction.\n",
    "                Если None, используется max timestamp из train_hist_df.\n",
    "    \"\"\"\n",
    "    df = df_candidates.copy()\n",
    "\n",
    "    # Средняя дата взаимодействия с книгами, которые пользователь прочитал\n",
    "    user_read_timestamps = (\n",
    "        train_hist_df[train_hist_df['has_read'] == 1]\n",
    "        .groupby('user_id')['timestamp']\n",
    "        .agg(['mean', 'max', 'min'])\n",
    "        .add_prefix('u_read_timestamp_')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(user_read_timestamps, on='user_id', how='left')\n",
    "\n",
    "    # Средняя дата взаимодействия с этой конкретной книгой\n",
    "    book_timestamps = (\n",
    "        train_hist_df.groupby('book_id')['timestamp']\n",
    "        .agg(['mean', 'max', 'min'])\n",
    "        .add_prefix('b_timestamp_')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(book_timestamps, on='book_id', how='left')\n",
    "\n",
    "    # --- Исправление: Преобразуем datetime в числовой формат (дни с 1970-01-01) ---\n",
    "    # Определяем базовую дату для преобразования\n",
    "    base_date = pd.Timestamp('1970-01-01')\n",
    "\n",
    "    # Преобразуем столбцы с датами в дни\n",
    "    temporal_cols_to_convert = [\n",
    "        'u_read_timestamp_mean', 'u_read_timestamp_max', 'u_read_timestamp_min',\n",
    "        'b_timestamp_mean', 'b_timestamp_max', 'b_timestamp_min'\n",
    "    ]\n",
    "\n",
    "    for col in temporal_cols_to_convert:\n",
    "        if col in df.columns:\n",
    "            # Проверяем, является ли столбец datetime\n",
    "            if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                # Преобразуем в дни от базовой даты\n",
    "                # .dt.total_seconds() / (24 * 3600) - это альтернатива .dt.days\n",
    "                df[col] = (df[col] - base_date).dt.days\n",
    "\n",
    "    # --- Конец исправления ---\n",
    "\n",
    "    # --- Исправление: Вычисляем days_since_last_interaction ---\n",
    "    # Определяем время \"предсказания\" для вычисления разницы\n",
    "    if split_time is None:\n",
    "        # Если split_time не задан, используем максимальное время из истории как приближение\n",
    "        prediction_time_days = (train_hist_df['timestamp'].max() - base_date).days\n",
    "    else:\n",
    "        prediction_time_days = (split_time - base_date).days\n",
    "\n",
    "    # Признак: сколько дней прошло с последнего взаимодействия с книгой\n",
    "    # (для cold candidates b_timestamp_max_days будет NaN)\n",
    "    # Вычисляем разницу между 'prediction_time_days' и 'b_timestamp_max_days'\n",
    "    # Предполагаем, что 'b_timestamp_max' уже был преобразован в дни выше\n",
    "    df['days_since_last_interaction'] = (prediction_time_days - df['b_timestamp_max']).fillna(10000)\n",
    "\n",
    "    # --- Конец исправления для days_since_last_interaction ---\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_collaborative_features(df_candidates, train_hist_df, n_factors=10):\n",
    "    \"\"\"\n",
    "    Добавляет признаки на основе матричной факторизации (SVD).\n",
    "    Это более сложный признак, но часто эффективный.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    df = df_candidates.copy()\n",
    "    \n",
    "    # Создание разреженной матрицы user-item (например, на основе has_read)\n",
    "    interaction_matrix = train_hist_df.pivot(index='user_id', columns='book_id', values='has_read').fillna(0)\n",
    "    \n",
    "    # Применение SVD\n",
    "    svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "    user_factors = svd.fit_transform(interaction_matrix)\n",
    "    item_factors = svd.components_.T\n",
    "\n",
    "    # Создание словарей для быстрого доступа к векторам\n",
    "    user_factors_dict = {uid: vec for uid, vec in zip(interaction_matrix.index, user_factors)}\n",
    "    item_factors_dict = {bid: vec for bid, vec in zip(interaction_matrix.columns, item_factors)}\n",
    "\n",
    "    # Функция для получения признаков\n",
    "    def get_user_factor_features(user_id):\n",
    "        return user_factors_dict.get(user_id, np.zeros(n_factors))\n",
    "    \n",
    "    def get_book_factor_features(book_id):\n",
    "        return item_factors_dict.get(book_id, np.zeros(n_factors))\n",
    "\n",
    "    # Применение\n",
    "    user_factor_features = df['user_id'].apply(get_user_factor_features).apply(pd.Series)\n",
    "    user_factor_features.columns = [f'user_factor_{i}' for i in range(n_factors)]\n",
    "    \n",
    "    book_factor_features = df['book_id'].apply(get_book_factor_features).apply(pd.Series)\n",
    "    book_factor_features.columns = [f'book_factor_{i}' for i in range(n_factors)]\n",
    "\n",
    "    df = pd.concat([df, user_factor_features, book_factor_features], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_popularity_trend_features(df_candidates, train_hist_df):\n",
    "    \"\"\"\n",
    "    Добавляет признаки, связанные с популярностью книги в разные временные периоды.\n",
    "    \"\"\"\n",
    "    df = df_candidates.copy()\n",
    "    \n",
    "    # Добавим колонку года для агрегации\n",
    "    train_hist_with_year = train_hist_df.copy()\n",
    "    train_hist_with_year['year'] = train_hist_with_year['timestamp'].dt.year\n",
    "    \n",
    "    # Популярность книги в последнем году\n",
    "    last_year = train_hist_with_year['year'].max()\n",
    "    book_popularity_last_year = (\n",
    "        train_hist_with_year[train_hist_with_year['year'] == last_year]\n",
    "        .groupby('book_id')['user_id']\n",
    "        .nunique()\n",
    "        .rename('book_popularity_last_year')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df = df.merge(book_popularity_last_year, on='book_id', how='left')\n",
    "    \n",
    "    # Заполняем NaN для книг, которые не были популярны в последнем году\n",
    "    df['book_popularity_last_year'] = df['book_popularity_last_year'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_genre_features(df_candidates, train_hist_df, books_metadata_df):\n",
    "    \"\"\"\n",
    "    Добавляет признаки, связанные с жанрами.\n",
    "    Требуется внешний датафрейм books_metadata_df с колонками ['book_id', 'genre'].\n",
    "    \"\"\"\n",
    "    df = df_candidates.copy()\n",
    "    \n",
    "    # Пример: совпадает ли жанр книги с \"предпочтительным\" жанром пользователя\n",
    "    # Предполагаем, что в books_metadata_df есть колонка 'genre'\n",
    "    df = df.merge(books_metadata_df[['book_id', 'genre']], on='book_id', how='left')\n",
    "    \n",
    "    # Находим \"предпочтительный\" жанр для каждого пользователя\n",
    "    user_preferred_genres = (\n",
    "        train_hist_df.merge(books_metadata_df[['book_id', 'genre']], on='book_id', how='left')\n",
    "        .groupby(['user_id', 'genre'])['book_id']\n",
    "        .count()\n",
    "        .groupby('user_id')\n",
    "        .idxmax()\n",
    "        .apply(lambda x: x[1]) # Получаем жанр\n",
    "        .rename('preferred_genre')\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    df = df.merge(user_preferred_genres, on='user_id', how='left')\n",
    "    df['genre_match'] = (df['genre'] == df['preferred_genre']).astype(int)\n",
    "    \n",
    "    # Заполняем NaN (например, если у пользователя не было взаимодействий или жанр неизвестен)\n",
    "    df['genre_match'] = df['genre_match'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_all_new_features(df_candidates, train_hist_df, books_metadata_df=None, split_time=None):\n",
    "    \"\"\"\n",
    "    Применяет все вышеуказанные функции для добавления признаков.\n",
    "    split_time: datetime - время разбиения, передаётся в add_temporal_features.\n",
    "    \"\"\"\n",
    "    df = df_candidates.copy()\n",
    "\n",
    "    df = add_interaction_features(df, train_hist_df)\n",
    "    # Передаём split_time в add_temporal_features\n",
    "    df = add_temporal_features(df, train_hist_df, split_time=split_time)\n",
    "    # df = add_collaborative_features(df, train_hist_df) # Закомментировано из-за сложности\n",
    "    df = add_popularity_trend_features(df, train_hist_df)\n",
    "    if books_metadata_df is not None:\n",
    "        df = add_genre_features(df, train_hist_df, books_metadata_df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faabdbae-6b5b-44a3-bae0-ae765da8f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 19\n",
      "Распределение rel в train_features_hist:\n",
      "rel\n",
      "0     90270\n",
      "1     89214\n",
      "2    126035\n",
      "Name: count, dtype: int64\n",
      "Распределение rel в val_features:\n",
      "rel\n",
      "0    59359\n",
      "1    23244\n",
      "2    30568\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Добавляем базовые признаки к тренировочным и валидационным данным\n",
    "train_features_hist = add_basic_features(train_candidates_hist, user_stats_hist, book_stats_hist)\n",
    "val_features = add_basic_features(val_candidates, user_stats_hist, book_stats_hist) # val_features, не val_features_hist\n",
    "\n",
    "# Добавляем новые признаки, используя исторический датасет train_hist_full\n",
    "# Важно: функция add_all_new_features определена в ячейке 28\n",
    "# Мы предполагаем, что метаданные книг (books_metadata_df) отсутствуют, поэтому передаём None\n",
    "# Если метаданные есть, их нужно загрузить и передать сюда.\n",
    "train_features_hist = add_all_new_features(train_features_hist, train_hist_full, books_metadata_df=None) # Исправлено имя функции, не merge\n",
    "val_features = add_all_new_features(val_features, train_hist_full, books_metadata_df=None) # Исправлено имя переменной, не val_features_hist, не merge\n",
    "\n",
    "# Выбираем итоговый список признаков\n",
    "feature_cols = [\n",
    "    c for c in train_features_hist.columns\n",
    "    if c not in ['user_id', 'book_id', 'rel']\n",
    "]\n",
    "\n",
    "# Подготовка данных для обучения\n",
    "X_train = train_features_hist[feature_cols]\n",
    "y_train_rel = train_features_hist['rel']\n",
    "y_train_read = (train_features_hist['rel'] == 2).astype(int)\n",
    "y_train_any = (train_features_hist['rel'] > 0).astype(int)\n",
    "\n",
    "X_val = val_features[feature_cols] # Используем обновлённый val_features с новыми признаками\n",
    "y_val_rel = val_features['rel']\n",
    "\n",
    "print(\"Число признаков:\", len(feature_cols))\n",
    "print(\"Распределение rel в train_features_hist:\")\n",
    "print(y_train_rel.value_counts().sort_index())\n",
    "print(\"Распределение rel в val_features:\")\n",
    "print(y_val_rel.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054804e0-229b-4c98-8784-9d36a8580201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример предсказаний (валидация):\n",
      "   user_id  book_id  rel      pred\n",
      "0  1551451  2573361    2  0.084771\n",
      "1  1397150  2538344    2  0.066607\n",
      "2  1358090  2019613    2  0.001704\n",
      "3   849910  2366271    2  0.040513\n",
      "4   849910  1716389    1  0.041120\n",
      "NDCG@20 на валидации: 0.94712\n"
     ]
    }
   ],
   "source": [
    "# Обучение моделей (берём код из ячейки 10, но используем обновлённые X_train, X_val)\n",
    "xgb_read_cv = xgb.XGBClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.025,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_any_cv = xgb.XGBClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.025,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=43,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_read_cv.fit(X_train, y_train_read)\n",
    "xgb_any_cv.fit(X_train, y_train_any)\n",
    "\n",
    "cb_read_cv = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    depth=8,\n",
    "    learning_rate=0.025,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "cb_any_cv = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    depth=8,\n",
    "    learning_rate=0.025,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=43\n",
    ")\n",
    "\n",
    "cb_read_cv.fit(X_train, y_train_read)\n",
    "cb_any_cv.fit(X_train, y_train_any)\n",
    "\n",
    "p_read_xgb_val = xgb_read_cv.predict_proba(X_val)[:, 1]\n",
    "p_any_xgb_val = xgb_any_cv.predict_proba(X_val)[:, 1]\n",
    "score_xgb_val = w1 * p_read_xgb_val + w2 * p_any_xgb_val\n",
    "\n",
    "p_read_cb_val = cb_read_cv.predict_proba(X_val)[:, 1]\n",
    "p_any_cb_val = cb_any_cv.predict_proba(X_val)[:, 1]\n",
    "score_cb_val = w1 * p_read_cb_val + w2 * p_any_cb_val\n",
    "\n",
    "# Добавляем предсказания в val_features ДО вычисления NDCG\n",
    "val_features['score_xgb'] = score_xgb_val\n",
    "val_features['score_cb'] = score_cb_val\n",
    "val_features['score_ens'] = (score_xgb_val + score_cb_val) / 2.0\n",
    "val_features['pred'] = val_features['score_ens'] # Создаём столбец 'pred'\n",
    "\n",
    "print(\"Пример предсказаний (валидация):\")\n",
    "print(val_features[['user_id', 'book_id', 'rel', 'pred']].head())\n",
    "\n",
    "# Теперь вычисляем NDCG\n",
    "ndcg20 = mean_ndcg(val_features, k=20) # val_features теперь содержит 'pred'\n",
    "print(f\"NDCG@20 на валидации: {ndcg20:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a934433-c323-4f7a-a481-44f226dcebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train_candidates_full: (378396, 3)\n",
      "Распределение rel в train_candidates_full:\n",
      "rel\n",
      "0    109335\n",
      "1    112458\n",
      "2    156603\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_full = train.copy()\n",
    "train_full['rel'] = np.where(train_full['has_read'] == 1, 2, 1)\n",
    "train_pos_full = train_full[['user_id', 'book_id', 'rel']].drop_duplicates()\n",
    "\n",
    "user_hist_books_full, popular_books_full = build_history_and_popularity(train)\n",
    "train_users_full = train_full['user_id'].unique()\n",
    "train_cold_full = build_cold_candidates(train_users_full, user_hist_books_full, popular_books_full, n_cold=N_COLD)\n",
    "\n",
    "train_candidates_full = pd.concat([train_pos_full, train_cold_full], ignore_index=True)\n",
    "train_candidates_full = train_candidates_full.drop_duplicates(['user_id', 'book_id'])\n",
    "\n",
    "print(\"Размер train_candidates_full:\", train_candidates_full.shape)\n",
    "print(\"Распределение rel в train_candidates_full:\")\n",
    "print(train_candidates_full['rel'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f95d79d-93c0-4f97-8be2-39ff926c4b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример агрегатов по книгам (full):\n",
      "   book_id  n_interactions  n_read  n_plan  read_rate  plan_rate\n",
      "0       20             122     103      19   0.844262   0.155738\n",
      "1       35               1       1       0   0.999999   0.000000\n",
      "2       52               1       1       0   0.999999   0.000000\n",
      "3       54               7       5       2   0.714286   0.285714\n",
      "4       69               1       1       0   0.999999   0.000000\n",
      "Пример агрегатов по пользователям (full):\n",
      "   user_id  u_n_interactions  u_n_read  u_n_plan  u_read_share\n",
      "0      151                75        36        39      0.480000\n",
      "1      210                31         0        31      0.000000\n",
      "2      560                 6         0         6      0.000000\n",
      "3     1380                56        29        27      0.517857\n",
      "4     1850                77        38        39      0.493506\n"
     ]
    }
   ],
   "source": [
    "book_stats_full = (train\n",
    "    .groupby('book_id')\n",
    "    .agg(\n",
    "        n_interactions=('user_id', 'nunique'),\n",
    "        n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "book_stats_full['read_rate'] = book_stats_full['n_read'] / (book_stats_full['n_read'] + book_stats_full['n_plan'] + 1e-6)\n",
    "book_stats_full['plan_rate'] = book_stats_full['n_plan'] / (book_stats_full['n_read'] + book_stats_full['n_plan'] + 1e-6)\n",
    "\n",
    "user_stats_full = (train\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        u_n_interactions=('book_id', 'nunique'),\n",
    "        u_n_read=('has_read', lambda x: int((x == 1).sum())),\n",
    "        u_n_plan=('has_read', lambda x: int((x == 0).sum()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_stats_full['u_read_share'] = user_stats_full['u_n_read'] / (user_stats_full['u_n_interactions'] + 1e-6)\n",
    "\n",
    "print(\"Пример агрегатов по книгам (full):\")\n",
    "print(book_stats_full.head())\n",
    "print(\"Пример агрегатов по пользователям (full):\")\n",
    "print(user_stats_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5980108-1fe6-4d3d-a54b-6b15a0e020bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков (full): 19\n",
      "Распределение rel (full):\n",
      "rel\n",
      "0    109335\n",
      "1    112458\n",
      "2    156603\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_features_full = add_basic_features(train_candidates_full, user_stats_full, book_stats_full)\n",
    "train_features_full = add_all_new_features(train_features_full, train, books_metadata_df=None, split_time=train['timestamp'].max())\n",
    "\n",
    "feature_cols_full = [\n",
    "    c for c in train_features_full.columns\n",
    "    if c not in ['user_id', 'book_id', 'rel']\n",
    "]\n",
    "\n",
    "X_full = train_features_full[feature_cols_full]\n",
    "y_full_rel = train_features_full['rel']\n",
    "y_full_read = (train_features_full['rel'] == 2).astype(int)\n",
    "y_full_any = (train_features_full['rel'] > 0).astype(int)\n",
    "\n",
    "print(\"Число признаков (full):\", len(feature_cols_full))\n",
    "print(\"Распределение rel (full):\")\n",
    "print(y_full_rel.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373c9537-1908-4724-841b-bf551e2b2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальные модели (XGBoost + CatBoost) обучены на полном тренировочном датасете.\n"
     ]
    }
   ],
   "source": [
    "xgb_read_full = xgb.XGBClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.025,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_any_full = xgb.XGBClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.025,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=101,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_read_full.fit(X_full, y_full_read)\n",
    "xgb_any_full.fit(X_full, y_full_any)\n",
    "\n",
    "cb_read_full = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    depth=8,\n",
    "    learning_rate=0.025,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=100\n",
    ")\n",
    "\n",
    "cb_any_full = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    depth=8,\n",
    "    learning_rate=0.025,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False,\n",
    "    random_seed=101\n",
    ")\n",
    "\n",
    "cb_read_full.fit(X_full, y_full_read)\n",
    "cb_any_full.fit(X_full, y_full_any)\n",
    "\n",
    "print(\"Финальные модели (XGBoost + CatBoost) обучены на полном тренировочном датасете.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09f03bc-42e8-422b-b4fd-89480c12abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер candidates_raw: (3512, 2)\n",
      "Колонки candidates_raw: ['user_id', 'book_id_list']\n",
      "   user_id                                       book_id_list\n",
      "0      210  11936,254097,709075,840500,971259,1037723,1074...\n",
      "1     1380  8369,28302,145975,482934,625734,998313,1098150...\n",
      "2     2050  4902,8369,18790,308364,317849,460492,822326,86...\n",
      "3     2740  39221,112023,149611,162418,181062,317050,43565...\n",
      "4     4621  28638,28639,28642,28901,31479,307058,475353,57...\n",
      "Длинный формат candidates_long: (81048, 2)\n",
      "   user_id  book_id\n",
      "0      210    11936\n",
      "0      210   254097\n",
      "0      210   709075\n",
      "0      210   840500\n",
      "0      210   971259\n"
     ]
    }
   ],
   "source": [
    "candidates_path = \"./data/raw/candidates.csv\"\n",
    "candidates_raw = pd.read_csv(candidates_path)\n",
    "\n",
    "print(\"Размер candidates_raw:\", candidates_raw.shape)\n",
    "print(\"Колонки candidates_raw:\", candidates_raw.columns.tolist())\n",
    "print(candidates_raw.head())\n",
    "\n",
    "candidates_long = candidates_raw.copy()\n",
    "candidates_long['book_id_list'] = candidates_long['book_id_list'].fillna('').astype(str)\n",
    "candidates_long['book_id_list'] = candidates_long['book_id_list'].str.split(',')\n",
    "candidates_long = candidates_long.explode('book_id_list')\n",
    "candidates_long = candidates_long[candidates_long['book_id_list'].str.strip() != '']\n",
    "candidates_long['book_id'] = candidates_long['book_id_list'].str.strip().astype(int)\n",
    "candidates_long = candidates_long[['user_id', 'book_id']].drop_duplicates()\n",
    "\n",
    "print(\"Длинный формат candidates_long:\", candidates_long.shape)\n",
    "print(candidates_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c06457-1a57-40d3-b6bb-f36f733c8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер test_features после join'ов: (81048, 21)\n",
      "   user_id  book_id  u_n_interactions  u_n_read  u_n_plan  u_read_share  \\\n",
      "0      210    11936                31         0        31           0.0   \n",
      "1      210   254097                31         0        31           0.0   \n",
      "2      210   709075                31         0        31           0.0   \n",
      "3      210   840500                31         0        31           0.0   \n",
      "4      210   971259                31         0        31           0.0   \n",
      "\n",
      "   n_interactions  n_read  n_plan  read_rate  ...  has_interacted  \\\n",
      "0           396.0   375.0    21.0   0.946970  ...               0   \n",
      "1           360.0   325.0    35.0   0.902778  ...               0   \n",
      "2           198.0   130.0    68.0   0.656566  ...               0   \n",
      "3            91.0    70.0    21.0   0.769231  ...               0   \n",
      "4             1.0     0.0     1.0   0.000000  ...               0   \n",
      "\n",
      "   user_book_rating  u_read_timestamp_mean  u_read_timestamp_max  \\\n",
      "0                 0                    NaN                   NaN   \n",
      "1                 0                    NaN                   NaN   \n",
      "2                 0                    NaN                   NaN   \n",
      "3                 0                    NaN                   NaN   \n",
      "4                 0                    NaN                   NaN   \n",
      "\n",
      "   u_read_timestamp_min  b_timestamp_mean  b_timestamp_max  b_timestamp_min  \\\n",
      "0                   NaN           17439.0          18869.0          14253.0   \n",
      "1                   NaN           17403.0          18873.0          14253.0   \n",
      "2                   NaN           17456.0          18864.0          16163.0   \n",
      "3                   NaN           18572.0          18875.0          17547.0   \n",
      "4                   NaN           17748.0          17748.0          17748.0   \n",
      "\n",
      "   days_since_last_interaction  book_popularity_last_year  \n",
      "0                          7.0                       52.0  \n",
      "1                          3.0                       35.0  \n",
      "2                         12.0                       26.0  \n",
      "3                          1.0                       60.0  \n",
      "4                       1128.0                        0.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Пример предсказаний на candidates_long:\n",
      "   user_id  book_id      pred\n",
      "0      210    11936  0.000030\n",
      "1      210   254097  0.001177\n",
      "2      210   709075  0.008729\n",
      "3      210   840500  0.020174\n",
      "4      210   971259  0.046294\n"
     ]
    }
   ],
   "source": [
    "test_features = add_basic_features(candidates_long, user_stats_full, book_stats_full)\n",
    "test_features = add_all_new_features(test_features, train, books_metadata_df=None, split_time=train['timestamp'].max())\n",
    "\n",
    "print(\"Размер test_features после join'ов:\", test_features.shape)\n",
    "print(test_features.head())\n",
    "\n",
    "X_test = test_features[feature_cols_full]\n",
    "\n",
    "p_read_xgb_test = xgb_read_full.predict_proba(X_test)[:, 1]\n",
    "p_any_xgb_test = xgb_any_full.predict_proba(X_test)[:, 1]\n",
    "score_xgb_test = w1 * p_read_xgb_test + w2 * p_any_xgb_test\n",
    "\n",
    "p_read_cb_test = cb_read_full.predict_proba(X_test)[:, 1]\n",
    "p_any_cb_test = cb_any_full.predict_proba(X_test)[:, 1]\n",
    "score_cb_test = w1 * p_read_cb_test + w2 * p_any_cb_test\n",
    "\n",
    "test_features['score_xgb'] = score_xgb_test\n",
    "test_features['score_cb'] = score_cb_test\n",
    "test_features['pred'] = (score_xgb_test + score_cb_test) / 2.0\n",
    "\n",
    "print(\"Пример предсказаний на candidates_long:\")\n",
    "print(test_features[['user_id', 'book_id', 'pred']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83c1b0f2-3acb-4812-b783-c94b2bf47f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример сабмита (формат A):\n",
      "   user_id                                       book_id_list\n",
      "0      210  3988468,3015694,1281035,971259,2447113,2225251...\n",
      "1     1380  2548861,2290484,482934,1326209,1098150,2379664...\n",
      "2     2050  1021078,460492,2053462,2254200,2465924,317849,...\n",
      "3     2740  987516,5535190,2327258,1834192,549194,2479424,...\n",
      "4     4621  1964216,2446687,2446685,1341761,2347564,234756...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'output\\submissions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(submission_user_list.head())\n\u001b[32m      6\u001b[39m submit_path = \u001b[33m\"\u001b[39m\u001b[33m./output/submissions/submission.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43msubmission_user_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmit_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mСабмит сохранён в:\u001b[39m\u001b[33m\"\u001b[39m, submit_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'output\\submissions'"
     ]
    }
   ],
   "source": [
    "submission_user_list = make_submission_user_list(test_features, top_k=20)\n",
    "\n",
    "print(\"Пример сабмита (формат A):\")\n",
    "print(submission_user_list.head())\n",
    "\n",
    "submit_path = \"./output/submissions/submission.csv\"\n",
    "submission_user_list.to_csv(submit_path, index=False)\n",
    "print(\"Сабмит сохранён в:\", submit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bce76-c15e-429d-badd-9841ce0cd84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
